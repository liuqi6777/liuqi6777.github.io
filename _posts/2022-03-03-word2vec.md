---
title: 'Notes for Word2vec'
date: 2022-03-03
permalink: /posts/2022/03/word2vec/
tags:
  - nlp
---

----

# Abstract

Word2vec is an efficient model to represent words in vector space. With this method, one word can be represented as a n-dim vector and we can see that it has some good property.

So how can we get these vectors? The core idea of Word2vec is to train on large scale corpus, and get the parameters of the model as the word representation. In this method, we define "Central word" and "Context", thinking of them as inputs and outputs.

The authors came up with two models: continuous bag-of-word model and skip-gram model. The former takes the context as input and the center word as output, while the latter does the opposite. Both of them work.

Besides, they also came up with some training skills, which can greatly improve training speed can effects.

# Models of Word2vec

## Continuous Bag-of-word(CBOW)

CBOW takes the context as input and predict a center word from the surrounding context.

<img src="https://s2.loli.net/2022/05/15/X76rntNIQ4D9zfB.png" alt="image.png" style="zoom: 33%;" />

Consider a CBOW model, we define

- ***Input vectors*** $\boldsymbol{x}^{(c)}$, which are both one-hot.
- ***Output vector*** $\boldsymbol{y}^{(c)}$, also a one-hot vector, representing the center word.
- ***Input word matrix*** $\boldsymbol{V} \in \mathbb{R}^{N \times |V|}$ and ***Output word matrix*** $\boldsymbol{U} \in \mathbb{R}^{|V|\times N}$. The $i$-th column of $\boldsymbol{V}$ is the $n$-dimensional embedded vector for word $w_i$ when it is an input to this model, denoted as $\boldsymbol{v}_i \in \mathbb{R}^{N}$. Similarly, the $j$-th row of $\boldsymbol{U}$ is an n-dimensional embedded vector for word $w_j$ when it is an output of the model, denoted as $\boldsymbol{u}_j \in \mathbb{R}^N$.

Now we can see how the model works.

1. Generate one-hot word vectors for the input context of size $m$: $(\boldsymbol{x}^{(c−m)} , \cdots , \boldsymbol{x}^{(c−1)} , \boldsymbol{x}^{(c+1)} , \cdots , \boldsymbol{x}^{(c+m)} \in \mathbb{R}^{|V|} )$.
2. Get embedded word vectors for the context 
   $$
   (\boldsymbol{v}_{c-m} = \boldsymbol{V}\boldsymbol{x}^{(c-m)}, \cdots, \boldsymbol{v}_{c+m} = \boldsymbol{V}\boldsymbol{x}^{(c+m)} \in \mathbb{R}^N)
   $$
   Actually we get the columns of $\boldsymbol{V}$​.
3. Average these vectors to get *hidden layer* $\boldsymbol{h} \in \mathbb{R}^N$.
4. Generate a score vector 
   $$
   \boldsymbol{z} = \boldsymbol{Uh} ∈ \mathbb{R}^{|V|}
   $$
   As the dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score.
5. Turn the scores into probabilities 
   $$
   \hat {\boldsymbol{y}} = \mathrm{softmax}(\boldsymbol{z}) \in \mathbb{R}^{|V|}
   $$
6. Calculate the *loss* using cross-entropy
   $$
   J = - \sum_{j=1}^{|V|} y_j \log \hat y_j
   $$
   Notice that the vector $\boldsymbol{y}$ is also one-hot, so we get
   $$
   J = - \log\hat y_j = -\log \frac{\exp(\boldsymbol{u}_j^T\boldsymbol{h})}{\sum_{j'=1}^{|V|}\exp(\boldsymbol{u}_{j'}^T\boldsymbol{h})} = - \boldsymbol{u}_j^T\boldsymbol{h} + \log \sum_{j'=1}^{|V|}\exp(\boldsymbol{u}_{j'}^T\boldsymbol{h})
   $$
7. Then we can use *stochastic gradient descent* to update all relevant word matrices $\boldsymbol{U}$ and $\boldsymbol{V}$.

## Skip-Gram

Another approach is to create a model such that given the center word, the model will be able to predict or generate the surrounding words. Here we call the word "jumped" the context. We call this type of model a Skip-Gram model.

<img src="https://s2.loli.net/2022/05/15/Rda7MsYEprmFBPe.png" alt="image.png" style="zoom:33%;" />

Let’s discuss the Skip-Gram model above.

- The setup is largely the same but we essentially swap our $\boldsymbol{x}$ and $\boldsymbol{y}$. The only one input one-hot vector (center word) we will represent with an $\boldsymbol{x}$. And the output vectors as $\boldsymbol{y}^{(j)}$ .
- We define $\boldsymbol{V}$ and $\boldsymbol{U}$​ the same as in CBOW.

Like CBOW, we breakdown the way this model works in these 6 steps:

1. Generate our one hot input vector $\boldsymbol{x} \in \mathbb{R}^{|V|}$​ of the center word.
2. Get our embedded word vector for the center word $\boldsymbol{v}_c = \boldsymbol {Vx} \in \mathbb{R}^N$.
3. Generate a score vector $\boldsymbol{z} = \boldsymbol{Uv}_c$.
4. Turn the score vector into probabilities
   $$
   \hat{\boldsymbol{y}} = \mathrm{softmax}(\boldsymbol{z})
   $$
   Note that $(\hat{y}_{c-m}, \cdots, \hat{y}_{c-1}, \hat{y}_{c+1}, \cdots, \hat{y}_{c+m})$ are the probabilities of observing each context word.
5. Calculate the loss 
   $$
   \begin{aligned}
   J &= -\log P(w_{c-m}, \cdots, w_{c-1}, w_{c+1}, \cdots, w_{c+m} \mid w_c) \\
     &= -\log \prod^{2m}_{j=0, j\neq m} P(w_{c-m+j}\mid w_c) \\ 
     &= -\log \prod^{2m}_{j=0, j\neq m} \frac{\exp (\boldsymbol{u}_{c-m+j}^T \boldsymbol{v}_c)}{\sum_{k=1}^{|V|} \exp(\boldsymbol{u}_{k}^T \boldsymbol{v}_c)} \\
     &= -\sum^{2m}_{j=0, j\neq m} \boldsymbol{u}_{c-m+j}^T \boldsymbol{v}_c + 2m\log \sum_{k=1}^{|V|}\exp(\boldsymbol{u}_{k}^T \boldsymbol{v}_c)
   \end{aligned}
   $$
   Actually, it just calculate the cross-entropy between the probability vector $\hat{\boldsymbol{y}}$ and every one-hot vectors, and sum them.
6. With this objective function, we can compute the gradients with respect to the unknown parameters and at each iteration update them via Stochastic Gradient Descent.

# Training tricks of Word2vec

## Hierarchical SoftMax

Hierarchical softmax is an efficient way of computing softmax. The model uses a binary tree to represent all words in the vocabulary. The $V$ words must be leaf units of the tree. It can be proved that there are $V − 1$ inner units. For each leaf unit, there exists a unique path from the root to the unit; and this path is used to estimate the probability of the word represented by the leaf unit.

<img src="https://s2.loli.net/2022/05/15/wbDvIV9EGW8iOsx.png" alt="image.png" style="zoom:33%;" />

In the hierarchical softmax model, there is no output vector representation for words. Instead, each of the $V − 1$ inner units has an output vector $\boldsymbol{v}_{n(w,j)}$. And the probability of a word being the output word is defined as
$$
P(w = w_O) = \prod_{j=1}^{L(w)-1} \sigma([n(w, j+1) = \mathrm{ch}(n(w, j))] \cdot \boldsymbol{v}_{n(w,j)}^T \boldsymbol{h})
$$
where $\mathrm{ch}(n)$ is the *left child* of unit $n$, $[x]$ is a special function defined as
$$
[x] = \left\{
\begin{aligned}
&1\quad  &\mathrm{if} \ x \ \mathrm{is \ true;} \\
&-1\quad &\mathrm{otherwise.}
\end{aligned}
\right.
$$
At each inner unit, we need to assign the probabilities of going left and going right.
$$
P(n, \mathrm{left}) = \sigma(\boldsymbol{v}_n^T\cdot \boldsymbol{h})\\
P(n , \mathrm{right}) = 1 - P(n, \mathrm{left}) = \sigma( - \boldsymbol{v}_n^T\cdot \boldsymbol{h})
$$
Then we can compute the probability of $w_2$ being the output word as
$$
\begin{aligned}
P(w_2 = w_O)
&= P(n(w_2, 1), \mathrm{left}) \cdot P(n(w_2, 2), \mathrm{left}) \cdot P(n(w_2, 3), \mathrm{right}) \\ 
&= \sigma(\boldsymbol{v}_{n(w_2, 1)}^T\boldsymbol{h}) \cdot \sigma(\boldsymbol{v}_{n(w_2, 2)}^T\boldsymbol{h}) \cdot \sigma(-\boldsymbol{v}_{n(w_3, 1)}^T\boldsymbol{h}) 
\end{aligned}
$$
It's not hard to vertify that
$$
\sum_{i=1}^{|V|}P(w_i = w_O) = 1
$$
To train the model, our goal is still to minimize the negative log likelihood $− \log P(w = w_O)$. But instead of updating output vectors per word, we update the vectors of the nodes in the binary tree that are in the path from root to leaf node.

## Negative Sampling

The idea of negative sampling is to only update a sample of them. 

Apparently the output word (*positive sample*) should be kept in our sample and gets updated, and we need to sample a few words as *negative samples* (hence “negative sampling”). A probabilistic distribution is needed for the sampling process, and it can be arbitrarily chosen. We call this distribution the noise distribution, and denote it as $P_n(w)$.

In word2vec, instead of using a form of negative sampling that produces a well-defined posterior multinomial distribution, the authors argue that the following simplified training objective is capable of producing high-quality word embeddings:
$$
J = - \log \sigma(\boldsymbol{v}_{w_O}^T\cdot \boldsymbol{h}) - \sum_{w_j \in \mathcal{W}_{neg}} \log \sigma(\boldsymbol{v}_{w_j}^T\cdot \boldsymbol{h})
$$
where $w_O$ is the output word (i.e. the positive sample), $\mathcal{W}_{neg} = \{w_j\mid j=1, \cdots, K\}$ is the set of words that are asmpled based on $P_n(w)$, i.e. negative samples.

# Other works about word embeddings

- Latent Semantic Analysis (LSA)

- Global Vectors (GloVe)

# Reference

> Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space.
>
> Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed representations of words and phrases and their compositionality. 
>
> Rong, X. (2014). word2vec parameter learning explained.
>
> [Stanford CS 224N](http://web.stanford.edu/class/cs224n/index.html)
>
> [秒懂词向量Word2vec的本质](https://zhuanlan.zhihu.com/p/26306795)

